# -*- coding: utf-8 -*-
"""Untitled52.ipynb의 사본

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v5WzwBTVv-qi50AfFjs4GnA1kU0NScxs
"""

# !pip install "numpy<2.0"
# !pip install "transformers>=4.45.0" "accelerate>=0.34.0"
# !pip install chromadb
# !pip install sentence-transformers

# Commented out IPython magic to ensure Python compatibility.
# !sudo apt-get install git-lfs -y
# !git lfs install

# !git clone https://github.com/aivle-agent/complaint_system_AI.git
# # %cd complaint_system_AI

# !git lfs pull

# !ls

# !unzip complain_data.zip -d data
# !ls data

# !gunzip -k prec_parallel.jsonl.gz

import os
import re
import json
import logging
import pickle
import pandas as pd
import numpy as np
import torch
import chromadb
from tqdm import tqdm
from typing import Any, List, Dict, Tuple, Optional
from sentence_transformers import SentenceTransformer
from sklearn.cluster import DBSCAN, MiniBatchKMeans, KMeans
from sklearn.metrics import silhouette_score
from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM
from chromadb import PersistentClient



def sanitize_text(x: Any) -> str:
    if not isinstance(x, str):
        return ""

    text = x
    text = re.sub(r"\d{2,3}-\d{3,4}-\d{4}", "[TEL]", text)
    text = re.sub(r"\d{6}-\d{7}", "[RRN]", text)
    text = re.sub(r"[A-Za-z0-9\._%+-]+@[A-Za-z0-9\.-]+\.[A-Za-z]{2,}", "[EMAIL]", text)
    text = re.sub(r"\d{10,}", "[NUMSEQ]", text)
    text = re.sub(r"[가-힣]{2,3}씨", "[NAME]", text)
    return text.strip()

# 2. 지방·중앙 행정기관 데이터 파싱 (Q/A 구분)


def parse_question_answer(full_text: str) -> Tuple[str, str]:
    if not isinstance(full_text, str):
        return "", ""

    text = full_text.strip()

    a_idx = (
        text.find("\nA :") if "\nA :" in text
        else text.find("A :") if "A :" in text
        else -1
    )

    if a_idx != -1:
        q_part = text[:a_idx].strip()
        a_part = text[a_idx:].strip()
        a_part = re.sub(r"^A\s*:\s*", "", a_part).strip()
    else:
        q_part, a_part = text, ""

    q_part = re.sub(r"^제목\s*:\s*", "", q_part)
    q_part = re.sub(r"^Q\s*:\s*", "", q_part)

    return sanitize_text(q_part), sanitize_text(a_part)


def load_qa_from_consulting_csv(csv_path: str, text_col="consulting_content") -> List[Dict[str, str]]:
    df = pd.read_csv(csv_path, encoding="utf-8-sig")

    if text_col not in df.columns:
        raise ValueError(f"{csv_path}에 {text_col} 컬럼 없음")

    data = []
    for _, row in df.iterrows():
        q, a = parse_question_answer(row[text_col])
        if len(q) >= 5 and len(a) >= 5:
            data.append({"question": q, "answer": a})

    print(f"[LOAD] {csv_path} → {len(data)} Q/A")
    return data


# 3. 국민신문고 데이터 (question only)


def load_novel_test_from_sinmungo(csv_path: str, text_col="content_full") -> List[Dict[str, str]]:
    df = pd.read_csv(csv_path, encoding="utf-8-sig")

    if text_col not in df.columns:
        raise ValueError(f"{csv_path}에 {text_col} 컬럼 없음")

    data = []
    for _, row in df.iterrows():
        q = sanitize_text(row[text_col])
        if len(q) >= 20:
            data.append({"question": q, "answer": ""})

    print(f"[LOAD] {csv_path} → {len(data)} items (novel test)")
    return data


# 4. Train / Test 데이터 구성


def build_train_and_novel_sets_raw():
    train_files = [
        ("data/지방행정기관.csv", "consulting_content"),
        ("data/중앙행정기관.csv", "consulting_content"),
    ]

    train_data = []
    for path, col in train_files:
        train_data.extend(load_qa_from_consulting_csv(path, col))

    novel_test_data = load_novel_test_from_sinmungo("data/국민신문고.csv")

    return train_data, novel_test_data

# 5. Deduplication


def deduplicate_train(train_data):
    df = pd.DataFrame(train_data)
    df["pair_count"] = df.groupby(["question", "answer"])["answer"].transform("count")
    df_unique = df.drop_duplicates(["question", "answer"]).reset_index(drop=True)
    print(f"[DEDUP] Train {len(df)} → {len(df_unique)} unique")
    return df_unique.to_dict(orient="records")


def deduplicate_novel(novel):
    df = pd.DataFrame(novel)
    df["q_count"] = df.groupby("question")["question"].transform("count")
    df_unique = df.drop_duplicates("question").reset_index(drop=True)
    print(f"[DEDUP] Novel {len(df)} → {len(df_unique)} unique")
    return df_unique.to_dict(orient="records")


# 6. 클러스터링


def cluster_train_questions(train_data, eps=0.3, min_samples=5):
    model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

    questions = [d["question"] for d in train_data]
    embeddings = model.encode(questions, convert_to_numpy=True, show_progress_bar=True)

    db = DBSCAN(eps=eps, min_samples=min_samples, metric="cosine")
    labels = db.fit_predict(embeddings)

    for d, lab in zip(train_data, labels):
        d["cluster_id"] = int(lab)

    print(f"[CLUSTER] clusters = {len(set(labels)) - (1 if -1 in labels else 0)}")
    print(f"[CLUSTER] noise ratio = {(labels == -1).mean():.2%}")

    return train_data, labels

# 7. JSONL Export


def export_to_jsonl(train, novel, out_dir="export"):
    os.makedirs(out_dir, exist_ok=True)

    train_path = os.path.join(out_dir, "train_data.jsonl")
    novel_path = os.path.join(out_dir, "novel_test.jsonl")

    with open(train_path, "w", encoding="utf-8") as f:
        for row in train:
            f.write(json.dumps({
                "input": row["question"],
                "output": row["answer"],
                "cluster_id": int(row.get("cluster_id", -1)),
                "pair_count": int(row.get("pair_count", 1)),
            }, ensure_ascii=False) + "\n")

    with open(novel_path, "w", encoding="utf-8") as f:
        for row in novel:
            f.write(json.dumps({
                "input": row["question"],
                "output": "",
                "q_count": int(row.get("q_count", 1)),
            }, ensure_ascii=False) + "\n")

    print("\n[EXPORT DONE]")
    print(f"Train JSONL → {train_path}")
    print(f"Novel JSONL → {novel_path}")

# 8. MAIN 실행


if __name__ == "__main__":
    train_raw, novel_raw = build_train_and_novel_sets_raw()

    train_unique = deduplicate_train(train_raw)
    novel_unique = deduplicate_novel(novel_raw)

    train_clustered, labels = cluster_train_questions(train_unique)

    export_to_jsonl(train_clustered, novel_unique, out_dir="export")

# import json
# import os
# import pandas as pd

PREC_PATH = "prec_parallel.jsonl"
LOCAL_CSV = "data/지방행정기관.csv"
CENTRAL_CSV = "data/중앙행정기관.csv"

OUT_ITEMS = "processed_items.jsonl"


def extract_trimmed_text(text, max_len=400):
    text = str(text)
    if len(text) <= max_len * 2:
        return text
    return text[:max_len] + "\n" + text[-max_len:]


def load_prec():
    if not os.path.exists(PREC_PATH):
        print("[WARN] No prec JSONL.")
        return []
    items = []
    with open(PREC_PATH, encoding="utf-8") as f:
        for line in f:
            try:
                o = json.loads(line)
                t = o.get("content")
                if not t:
                    continue
                items.append({
                    "id": str(o.get("id")),
                    "text": extract_trimmed_text(t),
                    "meta": {
                        "type": "판례",
                        "case_name": o.get("case_name"),
                        "court": o.get("court"),
                        "date": o.get("date"),
                    }
                })
            except:
                continue
    return items


def load_csv(path, label, col="consulting_content"):
    if not os.path.exists(path):
        print(f"[WARN] No CSV: {path}")
        return []
    df = pd.read_csv(path)
    if col not in df.columns:
        print(f"[WARN] Column {col} missing in {path}")
        return []
    df = df.dropna(subset=[col])
    items = []
    for idx, row in df.iterrows():
        text = str(row[col]).strip()
        if text:
            items.append({
                "id": f"{label}_{idx}",
                "text": extract_trimmed_text(text),
                "meta": {"type": label, "row_idx": int(idx)},
            })
    return items


if __name__ == "__main__":
    items = []
    items += load_prec()
    items += load_csv(LOCAL_CSV, "지방행정기관")
    items += load_csv(CENTRAL_CSV, "중앙행정기관")

    print("[INFO] Total items:", len(items))

    with open(OUT_ITEMS, "w", encoding="utf-8") as f:
        for it in items:
            f.write(json.dumps(it, ensure_ascii=False) + "\n")

    print("[DONE] Saved:", OUT_ITEMS)

import json
# import numpy as np
# from transformers import AutoModel, AutoTokenizer
# import torch
# import os
# from tqdm import tqdm

IN_ITEMS = "processed_items.jsonl"
OUT_DIR = "emb_shards"
os.makedirs(OUT_DIR, exist_ok=True)

MODEL_NAME = "kakaocorp/kanana-nano-2.1b-embedding"

print("[INFO] Loading model…")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

DEVICE = model.device
BATCH = 32
SHARD_SIZE = 30000

def embed(batch_texts):
    enc = tokenizer(
        batch_texts,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    ).to(DEVICE)

    with torch.no_grad():
        out = model(
            input_ids=enc["input_ids"],
            attention_mask=enc["attention_mask"],
            pool_mask=enc["attention_mask"],
        )
    return out[0].cpu().numpy()


if __name__ == "__main__":
    items = [json.loads(line) for line in open(IN_ITEMS, encoding="utf-8")]
    total = len(items)

    shard_id = 0
    for start in range(0, total, SHARD_SIZE):
        end = min(start + SHARD_SIZE, total)
        shard_items = items[start:end]
        shard_embs = []

        print(f"[SHARD {shard_id}] items {start} ~ {end}")

        for i in tqdm(range(0, len(shard_items), BATCH)):
            batch = shard_items[i:i+BATCH]
            texts = [it["text"] for it in batch]
            emb = embed(texts)
            shard_embs.append(emb)

        shard_embs = np.vstack(shard_embs)
        np.save(f"{OUT_DIR}/emb_shard_{shard_id}.npy", shard_embs)

        with open(f"{OUT_DIR}/items_shard_{shard_id}.jsonl", "w", encoding="utf-8") as f:
            for it in shard_items:
                f.write(json.dumps(it, ensure_ascii=False) + "\n")

        print(f"[DONE] Saved SHARD {shard_id}")
        shard_id += 1

# import numpy as np
# import json
# import os
# from sklearn.cluster import MiniBatchKMeans, KMeans
# from sklearn.metrics import silhouette_score

EMB_DIR = "emb_shards"
OUT_LABELS = "cluster_labels.npy"
OUT_CLUSTER_MODEL = "cluster_model.pkl"

# import pickle

def load_embeddings():
    arrs = []
    for f in sorted(os.listdir(EMB_DIR)):
        if f.startswith("emb_shard_") and f.endswith(".npy"):
            arrs.append(np.load(os.path.join(EMB_DIR, f)))
    return np.vstack(arrs)


def choose_k(embs, k_list=[10, 15, 20, 25, 30], sample=2000):
    if len(embs) > sample:
        idx = np.random.choice(len(embs), sample, replace=False)
        sample_emb = embs[idx]
    else:
        sample_emb = embs

    best_k = None
    best_score = -1

    print("[INFO] Testing K candidates…")
    for k in k_list:
        km = KMeans(n_clusters=k, n_init=5)
        labels = km.fit_predict(sample_emb)
        score = silhouette_score(sample_emb, labels)
        print(f"K={k}, silhouette={score:.4f}")
        if score > best_score:
            best_k = k
            best_score = score

    print("[INFO] Best K:", best_k)
    return best_k


if __name__ == "__main__":
    embs = load_embeddings()
    print("[INFO] EMB shape:", embs.shape)

    best_k = choose_k(embs)

    print("[INFO] Fitting MiniBatchKMeans…")
    mbk = MiniBatchKMeans(
        n_clusters=best_k,
        batch_size=2048,
        n_init=5,
        random_state=42
    )

    mbk.fit(embs)
    labels = mbk.predict(embs)

    np.save(OUT_LABELS, labels)

    with open(OUT_CLUSTER_MODEL, "wb") as f:
        pickle.dump(mbk, f)

    print("[DONE] Cluster saved.")

# import json
# import numpy as np
# import os
# import chromadb
# from chromadb.config import Settings

EMB_DIR = "emb_shards"
ITEM_DIR = "emb_shards"
CLUSTER_LABELS = "cluster_labels.npy"
DB_DIR = "./chroma_lawdb_kanana_clustered"

MAX_CHROMA_BATCH = 5000

os.makedirs(DB_DIR, exist_ok=True)

print("[INFO] Initializing ChromaDB")
client = chromadb.PersistentClient(path=DB_DIR)

try:
    client.delete_collection("law_corpus")
    print("[INFO] 기존 law_corpus 컬렉션 삭제")
except Exception:
    pass

coll = client.get_or_create_collection(
    "law_corpus",
    metadata={"hnsw:space": "cosine"},
    embedding_function=None,
)

labels = np.load(CLUSTER_LABELS)
label_ptr = 0

# shard_0, shard_1 ... 이런 순서 보장
shard_ids = sorted(set(
    int(f.split("_")[2].replace(".npy", ""))
    for f in os.listdir(EMB_DIR)
    if f.startswith("emb_shard_")
))

for shard_id in shard_ids:
    emb = np.load(f"{EMB_DIR}/emb_shard_{shard_id}.npy")
    items = [
        json.loads(line)
        for line in open(f"{ITEM_DIR}/items_shard_{shard_id}.jsonl", encoding="utf-8")
    ]

    assert emb.shape[0] == len(items), f"임베딩 개수와 아이템 개수가 다름: {emb.shape[0]} vs {len(items)}"

    print(f"[INFO] SHARD {shard_id}: inserting {len(items)} docs")

    # shard 전체를 먼저 리스트로 구성
    ids = []
    docs = []
    metas = []

    for i, it in enumerate(items):
        ids.append(it["id"])
        docs.append(it["text"])
        meta = dict(it["meta"])
        meta["cluster"] = int(labels[label_ptr])
        metas.append(meta)
        label_ptr += 1

    N = len(ids)
    for start in range(0, N, MAX_CHROMA_BATCH):
        end = min(start + MAX_CHROMA_BATCH, N)

        coll.add(
            ids=ids[start:end],
            embeddings=emb[start:end].tolist(),
            documents=docs[start:end],
            metadatas=metas[start:end],
        )

        print(f"[CHROMA] SHARD {shard_id} 저장: {end}/{N}")

print("[DONE] ChromaDB fully built:", DB_DIR)
print("[DONE] 총 라벨 사용 개수:", label_ptr)

# !zip -r chroma_lawdb_kanana_clustered.zip ./chroma_lawdb_kanana_clustered

# !cp -r ./chroma_lawdb_kanana_clustered /content/drive/MyDrive/

"""retriver"""

# retriever_kanana.py

# import os
# from typing import List, Dict, Any, Optional

# import numpy as np
# import chromadb
# from chromadb import PersistentClient

# import torch
# from transformers import AutoModel, AutoTokenizer


DB_DIR = os.environ.get("LAWDB_DIR", "./chroma_lawdb_kanana_clustered")
COLLECTION_NAME = "law_corpus"

# Kanana 임베딩 모델
EMBED_MODEL_NAME = "kakaocorp/kanana-nano-2.1b-embedding"

# 클러스터링 모델 (MiniBatchKMeans) 경로
CLUSTER_MODEL_PATH = os.environ.get("CLUSTER_MODEL_PATH", "./cluster_model.pkl")


# 1. Kanana embedding 모델 로드


print(f"[RAG] Loading Kanana embedding model: {EMBED_MODEL_NAME}")
_embed_tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_NAME)
_embed_model = AutoModel.from_pretrained(
    EMBED_MODEL_NAME,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
)
_EMBED_DEVICE = _embed_model.device
print(f"[RAG] Kanana device = {_EMBED_DEVICE}")


def get_query_embedding(text: str, max_length: int = 512) -> np.ndarray:
    """질문/민원 텍스트를 Kanana 임베딩 벡터로 변환"""
    enc = _embed_tokenizer(
        [text],
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors="pt",
    ).to(_EMBED_DEVICE)

    with torch.no_grad():
        out = _embed_model(
            input_ids=enc["input_ids"],
            attention_mask=enc["attention_mask"],
            pool_mask=enc["attention_mask"],
        )
    # out[0] : (B, D) pooled embedding
    emb = out[0][0].cpu().numpy()  # (D,)
    return emb


# 2. 클러스터 모델 로드


import pickle

if not os.path.exists(CLUSTER_MODEL_PATH):
    raise FileNotFoundError(
        f"[RAG] cluster_model.pkl을 찾을 수 없습니다: {CLUSTER_MODEL_PATH}\n"
        f"→ RAG DB 구축 시 MiniBatchKMeans 모델을 pickle로 저장했는지 확인하세요."
    )

print(f"[RAG] Loading cluster model from {CLUSTER_MODEL_PATH}")
with open(CLUSTER_MODEL_PATH, "rb") as f:
    _cluster_model = pickle.load(f)

_CLUSTER_CENTERS = _cluster_model.cluster_centers_


def get_top_clusters(q_emb: np.ndarray, top_m: int = 2) -> List[int]:
    """쿼리 임베딩과 가장 가까운 클러스터 top_m개 선택"""
    dists = np.linalg.norm(_CLUSTER_CENTERS - q_emb[None, :], axis=1)
    idx = np.argsort(dists)[:top_m]
    return [int(i) for i in idx]


# 3. ChromaDB 연결


print(f"[RAG] Initializing ChromaDB client at {DB_DIR}")
_client: PersistentClient = chromadb.PersistentClient(path=DB_DIR)
_collection = _client.get_collection(COLLECTION_NAME)
print(f"[RAG] Using collection: {COLLECTION_NAME}")


# 4. MMR rerank (유사도 + 다양성)


def _cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
    denom = (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8)
    return float(np.dot(a, b) / denom)


def mmr_rerank(
    query_emb: np.ndarray,
    doc_embs: np.ndarray,
    lambda_mult: float = 0.7,
    top_k: int = 5,
) -> List[int]:
    """
    query_emb: (D,)
    doc_embs: (N, D)
    → MMR 점수 기반으로 선택된 doc index 리스트
    """
    N = doc_embs.shape[0]
    if N <= top_k:
        return list(range(N))

    sims_to_query = np.array([
        _cosine_sim(query_emb, doc_embs[i]) for i in range(N)
    ])

    selected: List[int] = []
    candidate = list(range(N))


    first = int(np.argmax(sims_to_query))
    selected.append(first)
    candidate.remove(first)

    while len(selected) < top_k and candidate:
        mmr_scores = []
        for idx in candidate:
            sim_q = sims_to_query[idx]
            sim_div = max(
                _cosine_sim(doc_embs[idx], doc_embs[j]) for j in selected
            )
            score = lambda_mult * sim_q - (1 - lambda_mult) * sim_div
            mmr_scores.append((score, idx))

        mmr_scores.sort(reverse=True, key=lambda x: x[0])
        best_idx = mmr_scores[0][1]
        selected.append(best_idx)
        candidate.remove(best_idx)

    return selected


# 5. 메인 Retriever 함수


def retrieve_with_kanana(
    question: str,
    top_k: int = 5,
    cluster_top_m: int = 2,
    overfetch: int = 4,
    restrict_type: Optional[str] = None,   #
) -> List[Dict[str, Any]]:
    """
    Kanana 기반 RAG 검색

    return:
      [
        {
          "doc": <문서 원문>,
          "meta": <메타데이터 dict>,
          "distance": <Chroma distance>,
          "embedding": <np.ndarray (D,)>,
        },
        ...
      ]
    """
    # 1) 쿼리 임베딩
    q_emb = get_query_embedding(question)

    # 2) 클러스터 라우팅
    cluster_ids = get_top_clusters(q_emb, top_m=cluster_top_m)

    where_filter: Dict[str, Any]
    if restrict_type:
        where_filter = {
            "$and": [
                {"cluster": {"$in": cluster_ids}},
                {"type": restrict_type},
            ]
        }
    else:
        where_filter = {"cluster": {"$in": cluster_ids}}

    # 3) Chroma
    n_results = top_k * overfetch

    res = _collection.query(
        query_embeddings=[q_emb.tolist()],
        n_results=n_results,
        where=where_filter,
        include=["documents", "metadatas", "distances", "embeddings"],
    )

    docs = res["documents"][0]
    metas = res["metadatas"][0]
    dists = res["distances"][0]
    embs = np.array(res["embeddings"][0])

    if len(docs) == 0:
        return []

    # 4) MMR rerank로 최종 top_k 선택
    selected_idxs = mmr_rerank(q_emb, embs, lambda_mult=0.7, top_k=top_k)

    results: List[Dict[str, Any]] = []
    for i in selected_idxs:
        results.append({
            "doc": docs[i],
            "meta": metas[i],
            "distance": dists[i],
            "embedding": embs[i],
        })
    return results

# 6. 간단 테스트용 진입점


if __name__ == "__main__":
    print("[RAG] Simple test mode")
    while True:
        q = input("\n질문을 입력하세요 (엔터만 입력 시 종료): ").strip()
        if not q:
            break

        results = retrieve_with_kanana(q, top_k=5)
        print(f"\n[검색 결과 상위 {len(results)}개] =======================")
        for i, r in enumerate(results):
            meta = r["meta"]
            snippet = r["doc"][:150].replace("\n", " ")
            print(f"- [{i+1}] type={meta.get('type')} cluster={meta.get('cluster')} dist={r['distance']:.4f}")
            print(f"  내용: {snippet}...")

"""generator"""

# !unzip lawdb.zip -d /content/

# ============================================
# 민원 자동답변 시스템 (RAG + Multi-View + Critic + Self-Refine)
# - RAG: Kanana 2.1B embedding 기반 ChromaDB
# - Generator / Critic: kakaocorp/kanana-nano-2.1b-instruct
# - 출력: 최종 '검토내용' 답변 1개
# ============================================

# import os
# import json
# import logging
# from typing import List, Dict, Any, Tuple

# import numpy as np
# import torch
# from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
# import chromadb

# --------------------------------------------
# 0. 기본 설정
# --------------------------------------------

CHROMA_DIR = "/content/complaint_system_AI/chroma_lawdb_kanana_clustered"
EMBED_MODEL_NAME = "kakaocorp/kanana-nano-2.1b-embedding"
GEN_MODEL_NAME = "kakaocorp/kanana-nano-2.1b-instruct"
MEMORY_PATH = "./answer_memory.json"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"[INIT] DEVICE = {DEVICE}")

# --------------------------------------------
# 1. ChromaDB 초기화 (Kanana 임베딩 기반 DB)
# --------------------------------------------

logger.info("[INIT] Loading ChromaDB...")
client = chromadb.PersistentClient(path=CHROMA_DIR)

collections = client.list_collections()
if not collections:
    raise RuntimeError(f"[RAG] No collections found in {CHROMA_DIR}")

collection = client.get_collection(collections[0].name)
logger.info(f"[RAG] Using collection: {collections[0].name}")

# --------------------------------------------
# 2. Kanana 임베딩 모델 (RAG query용)
# --------------------------------------------

logger.info(f"[INIT] Loading Kanana embedding model: {EMBED_MODEL_NAME}")
emb_tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_NAME)
emb_model = AutoModel.from_pretrained(
    EMBED_MODEL_NAME,
    trust_remote_code=True,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
)
emb_model.eval()

@torch.no_grad()
def embed_kanana_batch(texts: List[str], max_length: int = 512) -> np.ndarray:
    """
    Kanana embedding 모델용 배치 임베딩 함수.
    (DB 생성 때 썼던 것과 동일한 방식 사용)
    """
    enc = emb_tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors="pt",
    ).to(emb_model.device)

    pool_mask = enc["attention_mask"]
    outputs = emb_model(
        input_ids=enc["input_ids"],
        attention_mask=enc["attention_mask"],
        pool_mask=pool_mask,
    )
    # 모델 카드 기준: pool_mask 주면 outputs[0]이 pooled embedding (B, D)
    pooled = outputs[0]
    return pooled.cpu().numpy()

def embed_kanana_single(text: str) -> List[float]:
    return embed_kanana_batch([text])[0].tolist()

# --------------------------------------------
# 3. Generator / Critic 모델 (kanana instruct)
# --------------------------------------------

logger.info(f"[INIT] Loading Generator/Critic model: {GEN_MODEL_NAME}")
gen_tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL_NAME)
if gen_tokenizer.pad_token is None:
    gen_tokenizer.pad_token = gen_tokenizer.eos_token

gen_model = AutoModelForCausalLM.from_pretrained(
    GEN_MODEL_NAME,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
)
gen_model.eval()

# --------------------------------------------
# 4. 공통 유틸: 요약 + Few-shot 템플릿
# --------------------------------------------

def summarize_question(question: str, max_len: int = 180) -> str:
    q = question.strip().replace("\n", " ")
    return q[:max_len] + ("..." if len(q) > max_len else "")

FEW_SHOT_TEMPLATE = """
아래는 공공 민원에 대한 모범 답변 예시입니다.
출력에서는 절대로 예시나 설명을 포함하지 말고,
오직 '검토내용'으로 시작하는 정식 답변만 작성하십시오.

※ 중요 규칙:
- 출력은 반드시 '검토내용'으로 시작해야 합니다.
- 검토내용 이후에는 번호 형태의 항목만 작성하십시오.
- '요약', '설명', '참고자료', '해설', '추가 안내', '요약문' 등 어떤 형태의 부가 설명도 절대로 작성하지 마십시오.
- 예시 문장, 프롬프트 내용, 분석 내용은 출력 금지.
- 최종 출력은 오직 정식 답변만 포함해야 하며, 그 외 텍스트는 절대로 포함하지 않습니다.

[예시 1]
민원 요지 요약: 교차로 신호 조정 요청
정식 답변:
검토내용
1. 안녕하십니까. 귀하께서 제기하신 민원에 대해 답변드립니다.
2. 민원 요지: 교차로 신호 운영 조정 요청입니다.
3. 가. 관계 기관 협의 결과 야간 시간대 단계적 조정이 타당한 것으로 검토되었습니다.
   나. 이에 따라 24시~07시 점멸 운영을 우선 시행하고 모니터링 후 추가 조정 여부를 검토하겠습니다.
4. 교통과로 문의 바랍니다. 감사합니다.

[예시 2]
민원 요지 요약: 보행자 안전시설 확충 요청
정식 답변:
검토내용
1. 안녕하십니까. 귀하께서 제기하신 민원에 대해 답변드립니다.
2. 민원 요지: 안전시설 설치 요청입니다.
3. 가. 현장 점검 결과 보행량·속도 등을 고려할 때 시설 확충이 필요한 것으로 확인되었습니다.
   나. 상반기 내 우선 조치 후 사고 추이를 보아 추가 설치를 검토할 예정입니다.
4. 자세한 사항은 교통행정과로 문의 바랍니다. 감사합니다.

위 형식을 참고하여 아래 민원에 대한 정식 답변만 작성하십시오.
예시 문장·설명·프롬프트는 출력하지 마십시오.
"""

def strip_to_answer(full_text: str, prompt: str) -> str:
    """
    전체 생성 텍스트에서 프롬프트 제거하고 '검토내용'부터만 남겨서 리턴
    """
    text = full_text.replace(prompt, "").strip()
    idx = text.find("검토내용")
    if idx != -1:
        text = text[idx:]
    else:
        # 혹시 prefix가 없으면, '검토내용' 기준으로 split 시도
        parts = text.split("검토내용")
        if len(parts) > 1:
            text = "검토내용" + parts[-1]
    return text.strip()

# --------------------------------------------
# 5. RAG: 탐색 + View 생성
# --------------------------------------------

def rag_explore(question: str, top_k: int = 40) -> Tuple[List[str], List[Dict[str, Any]]]:
    """
    - Kanana 임베딩으로 query → Chroma top_k
    - cluster 다양성 확보: cluster별로 최대 n개씩
    """
    q_emb = embed_kanana_single(question)
    res = collection.query(
        query_embeddings=[q_emb],
        n_results=top_k,
    )
    docs = res["documents"][0]
    metas = res["metadatas"][0]

    per_cluster = 3
    bucket = {}
    picked_docs = []
    picked_meta = []

    for d, m in zip(docs, metas):
        if not isinstance(d, str):
            continue
        c = m.get("cluster", -1)
        bucket.setdefault(c, 0)
        if bucket[c] >= per_cluster:
            continue
        bucket[c] += 1
        picked_docs.append(d)
        picked_meta.append(m)

    logger.info(f"[RAG] Retrieved {len(picked_docs)} docs after cluster balancing")
    return picked_docs, picked_meta

@torch.no_grad()
def llm_summarize_block(title: str, question: str, docs: List[str]) -> str:
    if not docs:
        return ""
    ctx = "\n\n".join(d[:800] for d in docs[:8])
    prompt = f"""
아래 자료는 '{title}'에 해당하는 참고자료입니다.
민원 답변을 작성하는 데 필요한 핵심 내용만 7줄 이내로 요약하십시오.

[질문]
{question}

[참고자료]
{ctx}

[요약]
"""
    inputs = gen_tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=2048,
    ).to(gen_model.device)

    out = gen_model.generate(
        **inputs,
        max_new_tokens=300,
        pad_token_id=gen_tokenizer.eos_token_id,
    )
    txt = gen_tokenizer.decode(out[0], skip_special_tokens=True)
    if "[요약]" in txt:
        txt = txt.split("[요약]", 1)[-1]
    return txt.strip()

def build_views(question: str, docs: List[str]) -> Dict[str, str]:
    """
    docs → 법령 / 사례 / 종합 3개 view 요약 생성
    """
    law_docs = [d for d in docs if any(k in d for k in ["법", "조례", "시행령", "조항", "규정"])]
    case_docs = [d for d in docs if any(k in d for k in ["민원", "사례", "판결", "판례", "신청인", "피신청인"])]
    mixed_docs = docs[: min(len(docs), 12)]

    law_summary = llm_summarize_block("관련 법령 및 조항", question, law_docs)
    case_summary = llm_summarize_block("유사 사례 및 판례", question, case_docs)
    mixed_summary = llm_summarize_block("종합 참고자료", question, mixed_docs)

    return {
        "law": law_summary,
        "case": case_summary,
        "mixed": mixed_summary,
    }

# --------------------------------------------
# 6. 여러 View 기반 후보 답변 생성
# --------------------------------------------

GEN_STRATEGIES = [
    {"name": "law_focus",   "view": "law",   "temp": 0.6, "top_p": 0.9},
    {"name": "case_focus",  "view": "case",  "temp": 0.7, "top_p": 0.9},
    {"name": "mixed_focus", "view": "mixed", "temp": 0.8, "top_p": 0.95},
]

def build_answer_prompt(question: str, view_summary: str) -> str:
    return (
        FEW_SHOT_TEMPLATE
        + "\n민원 요지:\n"
        + summarize_question(question)
        + "\n\n[요약된 참고자료]\n"
        + (view_summary if view_summary else "별도의 참고자료 없음")
        + "\n\n위 내용을 바탕으로, '검토내용'으로 시작하는 공공 민원 답변을 작성하십시오.\n"
        + "예시 문장이나 설명은 출력하지 말고, 최종 답변만 작성하십시오.\n\n"
        + "검토내용\n"
    )

@torch.no_grad()
def generate_candidates(question: str, views: Dict[str, str]) -> List[Dict[str, Any]]:
    candidates = []
    for strat in GEN_STRATEGIES:
        view_name = strat["view"]
        view_summary = views.get(view_name, "")
        prompt = build_answer_prompt(question, view_summary)

        inputs = gen_tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048,
        ).to(gen_model.device)

        out = gen_model.generate(
            **inputs,
            max_new_tokens=600,
            do_sample=True,
            temperature=strat["temp"],
            top_p=strat["top_p"],
            pad_token_id=gen_tokenizer.eos_token_id,
        )

        full_text = gen_tokenizer.decode(out[0], skip_special_tokens=True)
        answer = strip_to_answer(full_text, prompt)

        candidates.append({
            "strategy": strat["name"],
            "view": view_name,
            "answer": answer,
        })

    logger.info(f"[GEN] Generated {len(candidates)} candidate answers")
    return candidates

# --------------------------------------------
# 7. Critic (LLM 기반 평가자) + Self-Refine
# --------------------------------------------

@torch.no_grad()
def critic_score(question: str, answer: str) -> Tuple[float, str]:
    """
    LLM으로 0.0~1.0 점수와 간단한 이유를 받는다.
    """
    prompt = f"""
다음 민원 답변을 0.0~1.0 사이 점수로 평가하십시오.

[질문]
{question}

[답변]
{answer}

평가 기준:
- 공공 민원 답변 형식(검토내용 1,2,3,4 구조) 준수 여부
- 민원 요지 반영 정도
- 검토 과정 및 조치 계획의 구체성
- 관련 법령·조례·규정 등의 적절한 언급 여부
- 공무원 답변 톤·표현의 적절성

출력 형식:
점수: <0.00~1.00>
설명: <짧은 이유>
"""
    inputs = gen_tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=2048,
    ).to(gen_model.device)

    out = gen_model.generate(
        **inputs,
        max_new_tokens=300,
        pad_token_id=gen_tokenizer.eos_token_id,
    )
    txt = gen_tokenizer.decode(out[0], skip_special_tokens=True)
    score = 0.5
    reason = ""

    for line in txt.splitlines():
        if "점수" in line:
            try:
                val = float(line.split(":", 1)[1].strip().split()[0])
                score = float(max(0.0, min(1.0, val)))
            except Exception:
                pass
        if line.startswith("설명"):
            reason = line.split(":", 1)[1].strip()

    return score, reason

@torch.no_grad()
def self_refine(question: str, base_answer: str, other_answers: List[str]) -> str:
    """
    1차 답변 + 다른 후보 요약을 보고 최종 답변으로 refine
    """
    others_block = "\n\n".join(
        f"- {a[:300].replace(chr(10), ' ')}" for a in other_answers if a
    )

    prompt = f"""
아래는 민원에 대한 1차 답변입니다. 그리고 참고할 수 있는 다른 후보 답변 일부입니다.
이들을 참고하여 더 완성도 높고 공공 민원 양식에 부합하는 최종 답변으로 수정하십시오.

[질문]
{question}

[1차 답변]
{base_answer}

[다른 후보 답변 요약 일부]
{others_block if others_block else "별도 후보 없음"}

요구사항:
- '검토내용'으로 시작하는 1,2,3,4 구조를 유지하십시오.
- 민원의 현황·문제점, 검토 결과, 향후 조치, 문의처를 명확히 구분하십시오.
- 예시 문장이나 지시문은 포함하지 마십시오.

[최종 답변]
검토내용
"""
    inputs = gen_tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=2048,
    ).to(gen_model.device)

    out = gen_model.generate(
        **inputs,
        max_new_tokens=700,
        pad_token_id=gen_tokenizer.eos_token_id,
    )
    txt = gen_tokenizer.decode(out[0], skip_special_tokens=True)
    if "[최종 답변]" in txt:
        txt = txt.split("[최종 답변]", 1)[-1]
    ans = strip_to_answer(txt, "검토내용")
    return ans

# --------------------------------------------
# 8. 메모리 로드
# --------------------------------------------

if os.path.exists(MEMORY_PATH):
    with open(MEMORY_PATH, "r", encoding="utf-8") as f:
        ANSWER_MEMORY: List[Dict[str, Any]] = json.load(f)
else:
    ANSWER_MEMORY: List[Dict[str, Any]] = []

def save_memory(record: Dict[str, Any]):
    ANSWER_MEMORY.append(record)
    with open(MEMORY_PATH, "w", encoding="utf-8") as f:
        json.dump(ANSWER_MEMORY, f, ensure_ascii=False, indent=2)

# --------------------------------------------
# 9. 최종 API: answer_question
# --------------------------------------------

# def answer_question(question: str, return_meta: bool = False):
#     """
#     - RAG 탐색 → view 요약
#     - view별 후보 답변 3개 생성
#     - critic으로 scoring, best 선택
#     - self-refine 두 번
#     - return_meta=False: 최종 답변(str)만 반환 (서비스용)
#     - return_meta=True: 내부 candidate / score / critic 로그도 같이 반환
#     """
#     # 1) RAG 탐색
#     docs, metas = rag_explore(question, top_k=40)

#     # 2) view 요약
#     views = build_views(question, docs)

#     # 3) 후보 답변 생성
#     candidates = generate_candidates(question, views)

#     # 4) critic 점수 부여
#     scored = []
#     for c in candidates:
#         s, reason = critic_score(question, c["answer"])
#         c_rec = {
#             **c,
#             "critic_score": s,
#             "critic_reason": reason,
#         }
#         scored.append(c_rec)

#     scored.sort(key=lambda x: x["critic_score"], reverse=True)

#     best = scored[0]
#     base_answer = best["answer"]
#     other_answers = [c["answer"] for c in scored[1:]]

#     # 5) self-refine 2회
#     refined_1 = self_refine(question, base_answer, other_answers)
#     refined_2 = self_refine(question, refined_1, other_answers)
#     final_answer = refined_2.strip()

#     # 6) 로그 저장
#     record = {
#         "question": question,
#         "views": views,
#         "candidates": scored,
#         "final_answer": final_answer,
#     }
#     save_memory(record)

#     if return_meta:
#         return record
#     else:
#         return final_answer

def summarize_final_answer(question: str, answer: str) -> str:
    """
    최종 답변을 요약합니다. (Placeholder implementation)
    """
    return answer[:200] + "..." if len(answer) > 200 else answer


def answer_question(question: str, return_meta: bool = False):
    """
    최종 반환:
        {
          "answer": <공식 답변>,
          "summary": <요약 줄글>
        }
    """

    # 1) RAG 탐색
    docs, metas = rag_explore(question, top_k=40)

    # 2) view 요약
    views = build_views(question, docs)

    # 3) 후보 답변 생성
    candidates = generate_candidates(question, views)

    # 4) critic 점수 부여
    scored = []
    for c in candidates:
        s, reason = critic_score(question, c["answer"])
        scored.append({
            **c,
            "critic_score": s,
            "critic_reason": reason,
        })

    scored.sort(key=lambda x: x["critic_score"], reverse=True)

    # 5) best → self refine
    best = scored[0]
    base_answer = best["answer"]
    other_answers = [c["answer"] for c in scored[1:]]

    refined_1 = self_refine(question, base_answer, other_answers)
    refined_2 = self_refine(question, refined_1, other_answers)

    final_answer = refined_2.strip()

    # 6) 요약 생성
    summary = summarize_final_answer(question, final_answer)

    # 7) 저장
    record = {
        "question": question,
        "views": views,
        "candidates": scored,
        "final_answer": final_answer,
        "summary": summary,
    }
    save_memory(record)

    # 8) 서비스용 결과
    if return_meta:
        return record
    else:
        return {
            "answer": final_answer,
            "summary": summary,
        }


# --------------------------------------------
# 10. CLI 테스트
# --------------------------------------------

# if __name__ == "__main__":
#     print("\n[실시간 민원 질의 모드] (엔터만 입력하면 종료)\n")
#     while True:
#         try:
#             q = input("질문을 입력하세요: ").strip()
#         except EOFError:
#             break

#         if not q:
#             print("종료합니다.")
#             break

#         ans = answer_question(q, return_meta=False)
#         print("\n" + "="*80)
#         print(ans)
#         print("="*80 + "\n")

if __name__ == "__main__":
    print("\n[실시간 민원 질의 모드] (엔터만 입력하면 종료)\n")
    while True:
        q = input("질문을 입력하세요: ").strip()
        if not q:
            print("종료합니다.")
            break

        out = answer_question(q)

        print("\n" + "=" * 80)
        print("[공식 답변]")
        print(out["answer"])
        print("\n[요약]")
        print(out["summary"])
        print("=" * 80 + "\n")
